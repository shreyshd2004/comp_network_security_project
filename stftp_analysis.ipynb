{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sTFTP vs TFTP vs SFTP — Results Analysis\n",
    "\n",
    "This notebook loads your benchmark CSV(s) from the repo (e.g., `results_*_cold_sftp.csv`) and produces:\n",
    "- Summary tables (median / p90 latency, median overhead, median goodput)\n",
    "- Comparison charts (matplotlib-only)\n",
    "- An exportable `summary_metrics.csv` for your report\n",
    "\n",
    "> **Tip:** Place this notebook in the same folder as your `results_*.csv` files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib defaults only (no seaborn)\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (8, 4.5),\n",
    "    \"axes.grid\": True,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Select which CSV(s) to analyze\n",
    "\n",
    "- By default, we auto-pick the **latest** `results_*_cold_sftp.csv` if present.\n",
    "- You can also point to any pattern (e.g., `'results_*.csv'`) to combine multiple runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a glob pattern. If you only want the latest cold SFTP experiment, leave as-is.\n",
    "pattern = \"results_*_cold_sftp.csv\"\n",
    "\n",
    "files = sorted(glob.glob(pattern))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No files matched pattern: {pattern}\")\n",
    "print(\"Files to load:\")\n",
    "for f in files:\n",
    "    print(\" -\", f)\n",
    "\n",
    "# Load & concat\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(df)} rows from {len(files)} file(s).\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clean & normalize columns\n",
    "- Convert `elapsed_s`, `overhead_pct`, `goodput_Mbps` to numeric\n",
    "- Drop rows where `elapsed_s` is missing (failed runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# Ensure types are numeric where applicable\n",
    "for col in [\"elapsed_s\", \"overhead_pct\", \"goodput_Mbps\"]:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "# Remove failed/NA runs for latency stats\n",
    "df_ok = df_clean.dropna(subset=[\"elapsed_s\"]).copy()\n",
    "\n",
    "print(\"Rows after dropping NA elapsed_s:\", len(df_ok))\n",
    "df_ok.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Summary metrics per (protocol, file)\n",
    "We compute:\n",
    "- `runs` — number of successful runs\n",
    "- `median_latency_s`, `p90_latency_s`\n",
    "- `median_overhead_pct`, `median_goodput_Mbps`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(grouped):\n",
    "    out = grouped.agg(\n",
    "        runs=(\"elapsed_s\", \"count\"),\n",
    "        median_latency_s=(\"elapsed_s\", \"median\"),\n",
    "        p90_latency_s=(\"elapsed_s\", lambda x: np.percentile(x, 90)),\n",
    "        median_overhead_pct=(\"overhead_pct\", \"median\"),\n",
    "        median_goodput_Mbps=(\"goodput_Mbps\", \"median\"),\n",
    "    ).reset_index()\n",
    "    return out\n",
    "\n",
    "summary = summarize(df_ok.groupby([\"proto\", \"file\"]))\n",
    "summary.sort_values([\"file\", \"proto\"], inplace=True)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the summary table\n",
    "This writes `summary_metrics.csv` next to the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = \"summary_metrics.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "print(\"Saved:\", summary_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Charts\n",
    "\n",
    "### A) Median Latency by Protocol and File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_pivot = summary.pivot(index=\"file\", columns=\"proto\", values=\"median_latency_s\").sort_index()\n",
    "ax = lat_pivot.plot(kind=\"bar\")\n",
    "ax.set_title(\"Median Latency by Protocol and File\")\n",
    "ax.set_ylabel(\"Latency (seconds)\")\n",
    "ax.set_xlabel(\"File\")\n",
    "ax.legend(title=\"Protocol\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Median On-Wire Overhead (%) by Protocol and File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovh_pivot = summary.pivot(index=\"file\", columns=\"proto\", values=\"median_overhead_pct\").sort_index()\n",
    "ax = ovh_pivot.plot(kind=\"bar\")\n",
    "ax.set_title(\"Median On-Wire Overhead (%) by Protocol and File\")\n",
    "ax.set_ylabel(\"Overhead (%)\")\n",
    "ax.set_xlabel(\"File\")\n",
    "ax.legend(title=\"Protocol\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Median Goodput (Mb/s) by Protocol and File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_pivot = summary.pivot(index=\"file\", columns=\"proto\", values=\"median_goodput_Mbps\").sort_index()\n",
    "ax = gp_pivot.plot(kind=\"bar\")\n",
    "ax.set_title(\"Median Goodput (Mb/s) by Protocol and File\")\n",
    "ax.set_ylabel(\"Goodput (Mb/s)\")\n",
    "ax.set_xlabel(\"File\")\n",
    "ax.legend(title=\"Protocol\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Latency (log scale) trend across file sizes\n",
    "Useful to highlight handshake impact on small files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for proto, g in df_ok.groupby(\"proto\"):\n",
    "    med = g.groupby(\"file\")[\"elapsed_s\"].median().sort_index()\n",
    "    plt.plot(med.index, med.values, marker=\"o\", label=proto)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Latency (log scale) across file sizes\")\n",
    "plt.ylabel(\"Latency (s, log scale)\")\n",
    "plt.xlabel(\"File\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Quick Interpretation Notes\n",
    "\n",
    "- **Latency:** Expect TFTP < sTFTP < SFTP for small files; SFTP includes SSH handshake and TCP setup per cold transfer.\n",
    "- **Overhead:** sTFTP overhead should be higher than TFTP (AEAD tag + framing). SFTP is typically highest due to SSH and TCP.\n",
    "- **Goodput:** For larger files, sTFTP should approach TFTP; SFTP often lags due to per-chunk framing and TCP behavior over short transfers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
